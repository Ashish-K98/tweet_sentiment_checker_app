{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb896502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Show full column content\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Optional: control how many rows/columns are shown\n",
    "# pd.set_option(\"display.max_rows\", 100)   # default is 10\n",
    "# pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bfd37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2815b26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 45615\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 12284\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "\n",
    "train_df = ds[\"train\"].to_pandas()\n",
    "val_df = ds[\"validation\"].to_pandas()\n",
    "test_df = ds[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b014b68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@user Alciato: Bee will invest 150 million in January, another 200 in the Summer and plans to bring Messi by 2017\"</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          text  \\\n",
       "0                      \"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"   \n",
       "1                                                         \"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ\"   \n",
       "2                 Sorry bout the stream last night I crashed out but will be on tonight for sure. Then back to Minecraft in pc tomorrow night.   \n",
       "3  Chase Headley's RBI double in the 8th inning off David Price snapped a Yankees streak of 33 consecutive scoreless innings against Blue Jays   \n",
       "4                           @user Alciato: Bee will invest 150 million in January, another 200 in the Summer and plans to bring Messi by 2017\"   \n",
       "\n",
       "   label  \n",
       "0      2  \n",
       "1      1  \n",
       "2      1  \n",
       "3      1  \n",
       "4      2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing.py\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "import spacy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import emoji\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower=True, remove_urls=True, remove_mentions=True):\n",
    "        self.lower = lower\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_mentions = remove_mentions\n",
    "        self.url_re = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "        self.mention_re = re.compile(r\"@\\w+\")\n",
    "    def clean(self, text: str) -> str:\n",
    "        if self.remove_urls:\n",
    "            text = self.url_re.sub(\"\", text)\n",
    "        if self.remove_mentions:\n",
    "            text = self.mention_re.sub(\"\", text)\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # remove hastages\n",
    "        text = re.sub(r\"#\", \"\", text)  # just remove '#' but keep word\n",
    "        \n",
    "        # 4. Convert emojis to text (using emoji library)\n",
    "        text = emoji.demojize(text, delimiters=(\" \", \" \"))  \n",
    "        \n",
    "        # 5. Normalize elongated words (soooo ‚Üí soo)\n",
    "        def reduce_lengthening(word):\n",
    "            return re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", word)  # keep max 2 repeats\n",
    "\n",
    "        text = \" \".join([reduce_lengthening(w) for w in text.split()])\n",
    "        # 6. Remove special characters (optional, keep only words/emojis)\n",
    "        text = re.sub(r\"[^a-zA-Z0-9_\\s]\", \"\", text)\n",
    "        # 7. Remove extra spaces\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def spacy_tokenize(self, text: str) -> List[str]:\n",
    "        doc = nlp(text)\n",
    "        tokens = [t.lemma_ for t in doc if not t.is_stop and not t.is_punct and not t.like_num and not t.like_url]\n",
    "        return tokens\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        cleaned = [self.clean(str(x)) for x in X]\n",
    "        # return joined tokens (TF-IDF vectorizer will handle splitting or we can pass pre-tokenized)\n",
    "        return [\" \".join(self.spacy_tokenize(t)) for t in cleaned]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10367a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokens: ['This', 'is', 'an', 'example', 'sentence', ',', 'with', 'numbers', '123', 'and', 'a', 'URL', 'https://example.com', '!']\n",
      "Stopwords: ['This', 'is', 'an', 'with', 'and', 'a']\n",
      "Punctuation: [',', '!']\n",
      "Numbers: ['123']\n",
      "URLs: ['https://example.com']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"This is an example sentence, with numbers 123 and a URL https://example.com!\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"All tokens:\", [token.text for token in doc])\n",
    "print(\"Stopwords:\", [token.text for token in doc if token.is_stop])\n",
    "print(\"Punctuation:\", [token.text for token in doc if token.is_punct])\n",
    "print(\"Numbers:\", [token.text for token in doc if token.like_num])\n",
    "print(\"URLs:\", [token.text for token in doc if token.like_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d8f6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this movie :smiling_face_with_heart-eyes::fire: but the ending was :crying_face:\n",
      "I love this movie  but the ending was \n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "text = \"I love this movie üòçüî• but the ending was üò¢\"\n",
    "\n",
    "# Convert emojis to text description\n",
    "text_with_desc = emoji.demojize(text)\n",
    "print(text_with_desc)\n",
    "# Output: \"I love this movie :smiling_face_with_heart_eyes: :fire: but the ending was :crying_face:\"\n",
    "\n",
    "# Remove emojis completely\n",
    "text_removed = emoji.replace_emoji(text, replace='')\n",
    "print(text_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625457b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# train(sys.argv[1])\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# e.g. python train.py data/train.csv\u001b[39;00m\n\u001b[32m    107\u001b[39m   \u001b[38;5;66;03m# e.g. python train.py data/train.csv\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(path_to_csv)\u001b[39m\n\u001b[32m     63\u001b[39m param_grid = [\n\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# {   # Logistic Regression\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m#     \"clf\": [models[\"logreg\"]],\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m     },\n\u001b[32m     79\u001b[39m ]\n\u001b[32m     81\u001b[39m gs = GridSearchCV(\n\u001b[32m     82\u001b[39m     pipeline,\n\u001b[32m     83\u001b[39m     param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     verbose=\u001b[32m2\u001b[39m\n\u001b[32m     88\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[43mgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest params:\u001b[39m\u001b[33m\"\u001b[39m, gs.best_params_)\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest model:\u001b[39m\u001b[33m\"\u001b[39m, gs.best_estimator_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashis\\OneDrive\\Desktop\\Data_Science_Projects\\3.NLP\\tweet_sentiment_analysis\\.venv\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashis\\OneDrive\\Desktop\\Data_Science_Projects\\3.NLP\\tweet_sentiment_analysis\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1012\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1013\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1014\u001b[39m     )\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1021\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1022\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashis\\OneDrive\\Desktop\\Data_Science_Projects\\3.NLP\\tweet_sentiment_analysis\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1570\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1571\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1572\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashis\\OneDrive\\Desktop\\Data_Science_Projects\\3.NLP\\tweet_sentiment_analysis\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    958\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    959\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    960\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    961\u001b[39m         )\n\u001b[32m    962\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    983\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    984\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    985\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    986\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    987\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashis\\OneDrive\\Desktop\\Data_Science_Projects\\3.NLP\\tweet_sentiment_analysis\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     69\u001b[39m config = get_config()\n\u001b[32m     70\u001b[39m iterable_with_config = (\n\u001b[32m     71\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     73\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashis\\OneDrive\\Desktop\\Data_Science_Projects\\3.NLP\\tweet_sentiment_analysis\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashis\\OneDrive\\Desktop\\Data_Science_Projects\\3.NLP\\tweet_sentiment_analysis\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ashis\\OneDrive\\Desktop\\Data_Science_Projects\\3.NLP\\tweet_sentiment_analysis\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "from config import MODEL_PATH, RANDOM_SEED\n",
    "from preprocessing import SpacyPreprocessor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Expects a CSV with columns: text, label\n",
    "    Uses Hugging Face datasets library\n",
    "    \"\"\"\n",
    "\n",
    "    ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "\n",
    "    train_df = ds[\"train\"].to_pandas()\n",
    "    val_df = ds[\"validation\"].to_pandas()\n",
    "    test_df = ds[\"test\"].to_pandas()\n",
    "\n",
    "    combined_train_val_df=pd.concat((train_df,val_df),axis=0)\n",
    "\n",
    "    return combined_train_val_df\n",
    "\n",
    "\n",
    "\n",
    "def train(path_to_csv: str):\n",
    "    dataset = load_data(path_to_csv)\n",
    "\n",
    "    X = dataset[\"text\"]\n",
    "    y = dataset[\"label\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    # return X\n",
    "\n",
    "# Define models\n",
    "    models = {\n",
    "        \"logreg\": LogisticRegression(max_iter=1000, random_state=RANDOM_SEED),\n",
    "        \"rf\": RandomForestClassifier(random_state=RANDOM_SEED),\n",
    "        \"xgb\": XGBClassifier(\n",
    "            eval_metric=\"mlogloss\",\n",
    "            use_label_encoder=False,\n",
    "            random_state=RANDOM_SEED\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"pre\", SpacyPreprocessor()),\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), max_features=30000)),\n",
    "        (\"clf\", LogisticRegression())  # placeholder\n",
    "    ])\n",
    "\n",
    "    param_grid = [\n",
    "        {   # Logistic Regression\n",
    "            \"clf\": [models[\"logreg\"]],\n",
    "            \"clf__C\": [0.1, 1.0, 5.0],\n",
    "        },\n",
    "        {   # Random Forest\n",
    "            \"clf\": [models[\"rf\"]],\n",
    "            \"clf__n_estimators\": [100, 300],\n",
    "            \"clf__max_depth\": [None, 20],\n",
    "        },\n",
    "        {   # XGBoost\n",
    "            \"clf\": [models[\"xgb\"]],\n",
    "            \"clf__n_estimators\": [200, 500],\n",
    "            \"clf__learning_rate\": [0.1, 0.3],\n",
    "            \"clf__max_depth\": [6, 10],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring=\"f1_macro\",\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best params:\", gs.best_params_)\n",
    "    print(\"Best model:\", gs.best_estimator_)\n",
    "\n",
    "    preds = gs.predict(X_val)\n",
    "    print(classification_report(y_val, preds))\n",
    "    print(\"Macro F1:\", f1_score(y_val, preds, average=\"macro\"))\n",
    "\n",
    "    # Save best estimator\n",
    "    joblib.dump(gs.best_estimator_, MODEL_PATH)\n",
    "    print(\"Saved model to\", MODEL_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    # train(sys.argv[1])\n",
    "    train(\"text\")  # e.g. python train.py data/train.csv\n",
    "      # e.g. python train.py data/train.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "203d4e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       \"QT @user In the original draft of the 7th boo...\n",
       "1       \"Ben Smith / Smith (concussion) remains out of...\n",
       "2       Sorry bout the stream last night I crashed out...\n",
       "3       Chase Headley's RBI double in the 8th inning o...\n",
       "4       @user Alciato: Bee will invest 150 million in ...\n",
       "                              ...                        \n",
       "1995    \"LONDON (AP) \"\" Prince George celebrates his s...\n",
       "1996    Harper's Worst Offense against Refugees may be...\n",
       "1997    Hold on... Sam Smith may do the theme to Spect...\n",
       "1998    Gonna watch Final Destination 5 tonight. I alw...\n",
       "1999    \"Interview with Devon Alexander \\\"\"\"\"Speed Kil...\n",
       "Name: text, Length: 47615, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca81a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Expects a CSV with columns: text, label\n",
    "    Uses Hugging Face datasets library\n",
    "    \"\"\"\n",
    "\n",
    "    ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "\n",
    "    train_df = ds[\"train\"].to_pandas()\n",
    "    val_df = ds[\"validation\"].to_pandas()\n",
    "    test_df = ds[\"test\"].to_pandas()\n",
    "\n",
    "    combined_train_val_df=pd.concat((train_df,val_df),axis=0)\n",
    "\n",
    "    return combined_train_val_df\n",
    "\n",
    "df=load_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80d4b314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So disappointed in wwe summerslam! I want to see john cena wins his 16th title'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label']==0].iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b4001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
