{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb896502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Show full column content\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Optional: control how many rows/columns are shown\n",
    "# pd.set_option(\"display.max_rows\", 100)   # default is 10\n",
    "# pd.set_option(\"display.max_columns\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bfd37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc5c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "\n",
    "train_df = ds[\"train\"].to_pandas()\n",
    "val_df = ds[\"validation\"].to_pandas()\n",
    "test_df = ds[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b176245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing.py\n",
    "\n",
    "import re\n",
    "from typing import List\n",
    "import spacy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import emoji\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower=True, remove_urls=True, remove_mentions=True):\n",
    "        self.lower = lower\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_mentions = remove_mentions\n",
    "        self.url_re = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "        self.mention_re = re.compile(r\"@\\w+\")\n",
    "    def clean(self, text: str) -> str:\n",
    "        if self.remove_urls:\n",
    "            text = self.url_re.sub(\"\", text)\n",
    "        if self.remove_mentions:\n",
    "            text = self.mention_re.sub(\"\", text)\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # remove hastages\n",
    "        text = re.sub(r\"#\", \"\", text)  # just remove '#' but keep word\n",
    "        \n",
    "        # 4. Convert emojis to text (using emoji library)\n",
    "        text = emoji.demojize(text, delimiters=(\" \", \" \"))  \n",
    "        \n",
    "        # 5. Normalize elongated words (soooo ‚Üí soo)\n",
    "        def reduce_lengthening(word):\n",
    "            return re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", word)  # keep max 2 repeats\n",
    "\n",
    "        text = \" \".join([reduce_lengthening(w) for w in text.split()])\n",
    "        # 6. Remove special characters (optional, keep only words/emojis)\n",
    "        text = re.sub(r\"[^a-zA-Z0-9_\\s]\", \"\", text)\n",
    "        # 7. Remove extra spaces\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def spacy_tokenize(self, text: str) -> List[str]:\n",
    "        doc = nlp(text)\n",
    "        tokens = [t.lemma_ for t in doc if not t.is_stop and not t.is_punct and not t.like_num and not t.like_url]\n",
    "        return tokens\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        cleaned = [self.clean(str(x)) for x in X]\n",
    "        # return joined tokens (TF-IDF vectorizer will handle splitting or we can pass pre-tokenized)\n",
    "        return [\" \".join(self.spacy_tokenize(t)) for t in cleaned]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10367a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"This is an example sentence, with numbers 123 and a URL https://example.com!\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"All tokens:\", [token.text for token in doc])\n",
    "print(\"Stopwords:\", [token.text for token in doc if token.is_stop])\n",
    "print(\"Punctuation:\", [token.text for token in doc if token.is_punct])\n",
    "print(\"Numbers:\", [token.text for token in doc if token.like_num])\n",
    "print(\"URLs:\", [token.text for token in doc if token.like_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "text = \"I love this movie üòçüî• but the ending was üò¢\"\n",
    "\n",
    "# Convert emojis to text description\n",
    "text_with_desc = emoji.demojize(text)\n",
    "print(text_with_desc)\n",
    "# Output: \"I love this movie :smiling_face_with_heart_eyes: :fire: but the ending was :crying_face:\"\n",
    "\n",
    "# Remove emojis completely\n",
    "text_removed = emoji.replace_emoji(text, replace='')\n",
    "print(text_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "from config import MODEL_PATH, RANDOM_SEED\n",
    "from preprocessing import SpacyPreprocessor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Expects a CSV with columns: text, label\n",
    "    Uses Hugging Face datasets library\n",
    "    \"\"\"\n",
    "\n",
    "    ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "\n",
    "    train_df = ds[\"train\"].to_pandas()\n",
    "    val_df = ds[\"validation\"].to_pandas()\n",
    "    test_df = ds[\"test\"].to_pandas()\n",
    "\n",
    "    combined_train_val_df=pd.concat((train_df,val_df),axis=0)\n",
    "\n",
    "    return combined_train_val_df\n",
    "\n",
    "\n",
    "\n",
    "def train(path_to_csv: str):\n",
    "    dataset = load_data(path_to_csv)\n",
    "\n",
    "    X = dataset[\"text\"]\n",
    "    y = dataset[\"label\"]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
    "    )\n",
    "\n",
    "    # return X\n",
    "\n",
    "# Define models\n",
    "    models = {\n",
    "        \"logreg\": LogisticRegression(max_iter=1000, random_state=RANDOM_SEED),\n",
    "        \"rf\": RandomForestClassifier(random_state=RANDOM_SEED),\n",
    "        \"xgb\": XGBClassifier(\n",
    "            eval_metric=\"mlogloss\",\n",
    "            use_label_encoder=False,\n",
    "            random_state=RANDOM_SEED\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"pre\", SpacyPreprocessor()),\n",
    "        (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), max_features=30000)),\n",
    "        (\"clf\", LogisticRegression())  # placeholder\n",
    "    ])\n",
    "\n",
    "    param_grid = [\n",
    "        {   # Logistic Regression\n",
    "            \"clf\": [models[\"logreg\"]],\n",
    "            \"clf__C\": [0.1, 1.0, 5.0],\n",
    "        },\n",
    "        {   # Random Forest\n",
    "            \"clf\": [models[\"rf\"]],\n",
    "            \"clf__n_estimators\": [100, 300],\n",
    "            \"clf__max_depth\": [None, 20],\n",
    "        },\n",
    "        {   # XGBoost\n",
    "            \"clf\": [models[\"xgb\"]],\n",
    "            \"clf__n_estimators\": [200, 500],\n",
    "            \"clf__learning_rate\": [0.1, 0.3],\n",
    "            \"clf__max_depth\": [6, 10],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring=\"f1_macro\",\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best params:\", gs.best_params_)\n",
    "    print(\"Best model:\", gs.best_estimator_)\n",
    "\n",
    "    preds = gs.predict(X_val)\n",
    "    print(classification_report(y_val, preds))\n",
    "    print(\"Macro F1:\", f1_score(y_val, preds, average=\"macro\"))\n",
    "\n",
    "    # Save best estimator\n",
    "    joblib.dump(gs.best_estimator_, MODEL_PATH)\n",
    "    print(\"Saved model to\", MODEL_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    # train(sys.argv[1])\n",
    "    train(\"text\")  # e.g. python train.py data/train.csv\n",
    "      # e.g. python train.py data/train.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203d4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca81a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Expects a CSV with columns: text, label\n",
    "    Uses Hugging Face datasets library\n",
    "    \"\"\"\n",
    "\n",
    "    ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "\n",
    "    train_df = ds[\"train\"].to_pandas()\n",
    "    val_df = ds[\"validation\"].to_pandas()\n",
    "    test_df = ds[\"test\"].to_pandas()\n",
    "\n",
    "    combined_train_val_df=pd.concat((train_df,val_df),axis=0)\n",
    "\n",
    "    return combined_train_val_df\n",
    "\n",
    "df=load_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4b314",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['label']==0].iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b4001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
